{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition on Ontonotes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import names,gazetteers \n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_f1_score, flat_classification_report\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "The data is available from the [UBC library](http://dvn.library.ubc.ca.ezproxy.library.ubc.ca/dvn/dv/UBCLDS/faces/study/StudyPage.xhtml?globalId=hdl:11272/DSBHN). In this task, I will work with a subset of it: [ontonotes_NE](https://github.com/krieya/Name-Entity_Recognition/blob/master/ontonotes_NE/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontonotes_path = \"./ontonotes_NE/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate three lists (`train_data`, `dev_data`, and `test_data`) which consist of the paths of all the NER datafiles from their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['train/', 'dev/', 'test/']\n",
    "train_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "datas = [train_data, dev_data, test_data]\n",
    "\n",
    "for path, data in zip(paths, datas):\n",
    "    p = ontonotes_path + path\n",
    "    data += [path + c for c in os.listdir(p)]\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert your data from the .name files to standard IOB (**I**nside-**O**utside-**B**eginning) tags for NER. Each line of the data file contains a sentence with XML tags indicating the named entities. For example, if the sentence contains a *GPE* tag such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " < ENAMEX TYPE=\"GPE\" > Hong Kong < /ENAMEX >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag for 'Hong' is *B-GPE* and 'Kong' is *I-GPE* (GPE stands for Geopolitical Entity). The function below reads in a sentence from the dataset and converts it to a list of tokens with corresponding IOB-tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2iob(sentence):\n",
    "    '''Input sentence is a string from the Ontonotes corpus, with xml tags indicating named enitites\n",
    "    output is a list of tokens and a list of NER IOB-tags corresponding to those tokens'''\n",
    "    # your code here\n",
    "    \n",
    "    if sentence.startswith('</DOC>') or sentence.startswith(\"\\n\"):\n",
    "        return [], []\n",
    "    \n",
    "    curr_tokens = []\n",
    "    curr_tags = []\n",
    "    split_list = re.split(\"<|>\", sentence)\n",
    "    name_yes = False\n",
    "    \n",
    "    for s in split_list:\n",
    "        if s == \"\":\n",
    "            continue\n",
    "        \n",
    "        if s.startswith(\"ENAMEX\"):\n",
    "            enamex_type = s.split('''TYPE=\"''')[1].split('\"')[0]\n",
    "            name_yes = True\n",
    "            \n",
    "        elif s == '/ENAMEX':\n",
    "            name_yes = False\n",
    "            \n",
    "        else:\n",
    "            tokens = s.strip().split(\" \")\n",
    "            if name_yes:\n",
    "                first_word = tokens.pop(0)\n",
    "\n",
    "                curr_tokens.append(first_word)\n",
    "                curr_tags.append(\"B-\" + enamex_type)\n",
    "                \n",
    "                if tokens:\n",
    "                    for word in tokens:\n",
    "                        if word == \" \" or word == \"\":\n",
    "                            continue\n",
    "                        curr_tokens.append(word)\n",
    "                        curr_tags.append(\"I-\" + enamex_type)\n",
    "\n",
    "            else:\n",
    "                for word in tokens:\n",
    "                    if word == \" \" or word == \"\":\n",
    "                        continue\n",
    "                    curr_tokens.append(word)\n",
    "                    curr_tags.append(\"O\")\n",
    "    return curr_tokens, curr_tags         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "check_sentence = 'While <ENAMEX TYPE=\"PERSON\">Galloway</ENAMEX> \\'s <ENAMEX TYPE=\"ORG\" S_OFF=\"4\">pro-Wal-Mart</ENAMEX> film introduces us to grateful employees /-'\n",
    "curr_tokens, curr_tags = sentence2iob(check_sentence)\n",
    "assert curr_tags == ['O', 'B-PERSON', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "check_sentence = '<ENAMEX TYPE=\"GPE\">Moscow</ENAMEX> , overcast changing to moderate snow , <ENAMEX TYPE=\"QUANTITY\">2 degrees below zero</ENAMEX> to <ENAMEX TYPE=\"QUANTITY\">1 degree</ENAMEX> .'\n",
    "curr_tokens, curr_tags = sentence2iob(check_sentence)\n",
    "assert curr_tags == ['B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-QUANTITY', 'I-QUANTITY', 'I-QUANTITY', 'I-QUANTITY', 'O', 'B-QUANTITY', 'I-QUANTITY', 'O']\n",
    "\n",
    "token_count = 0\n",
    "for filename in train_data:\n",
    "    with open(ontonotes_path + filename, encoding=\"utf-8\") as f:\n",
    "        f.readline()\n",
    "        for sentence in f:\n",
    "            curr_tokens, curr_tags = sentence2iob(sentence)\n",
    "            token_count += len(curr_tokens)\n",
    "            assert len(curr_tokens) == len(curr_tags)\n",
    "            assert \"\" not in curr_tokens # if you have empty strings, you've done something wrong\n",
    "\n",
    "assert token_count == 1096878\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification\n",
    "\n",
    "Train a simple Naive bayes classifer to perform NER. \n",
    "\n",
    "The quality of the model depends on utilizing informative features for our task. So the features below are used to boost the classfier performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaz_vocab_ori = set(gazetteers.words())\n",
    "gaz_vocab = set()\n",
    "\n",
    "for v in gaz_vocab_ori:\n",
    "    for v_s in v.split(\" \"):\n",
    "        gaz_vocab.add(v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names_vocab = set(names.words())\n",
    "female_names = set(names.words('female.txt'))\n",
    "male_names = set(names.words('male.txt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = '<pad>'\n",
    "\n",
    "def word2features(sentence, idx, postags):\n",
    "    word_features = {}\n",
    "    word_features['word_lowercase'] = sentence[idx].lower()\n",
    "    \n",
    "    padded_sent = [pad_token, pad_token] + [s for s in sentence] + [pad_token, pad_token]\n",
    "    \n",
    "    left_word2 = padded_sent[idx]\n",
    "    left_word = padded_sent[idx + 1]\n",
    "    right_word = padded_sent[idx + 3]\n",
    "    right_word2 = padded_sent[idx + 4]\n",
    "    \n",
    "    # Neighbouring word features\n",
    "       \n",
    "    word_features['skip_gram'] = left_word + \" \" + right_word\n",
    "    word_features['left_bigram'] = left_word + \" \" + sentence[idx]\n",
    "    word_features['right_bigram'] = sentence[idx] +\" \"+ right_word\n",
    "    word_features['left_trigram'] = \" \".join([left_word2, left_word, sentence[idx]])\n",
    "#     word_features['right_trigram'] = \" \".join([sentence[idx], right_word, right_word2])\n",
    "        \n",
    "    # Word shape\n",
    "    \n",
    "    if sentence[idx].istitle():\n",
    "        word_features['word_istitle'] = 1\n",
    "    else:\n",
    "        word_features['word_istitle'] = 0\n",
    "        \n",
    "    \n",
    "    if sentence[idx].isupper():\n",
    "        word_features['word_isupper'] = 1\n",
    "    else:\n",
    "        word_features['word_isupper'] = 0\n",
    "        \n",
    "    if sentence[idx].isdigit():\n",
    "        word_features['word_isdigit'] = 1\n",
    "    else:\n",
    "        word_features['word_isdigit'] = 0\n",
    "    \n",
    "    # Subword feature\n",
    "    \n",
    "    word_features['subword_pre3'] = sentence[idx][:3]\n",
    "    word_features['subword_end3'] = sentence[idx][-3:]\n",
    "    \n",
    "    word_features['subword_pre2'] = sentence[idx][:2]\n",
    "    word_features['subword_end2'] = sentence[idx][-2:]\n",
    "    \n",
    "    word_features['left_subword_pre3'] = left_word[:3]\n",
    "    word_features['left_subword_end3'] = left_word[-3:]\n",
    "    \n",
    "    # gazetteer feature\n",
    "    \n",
    "    \n",
    "    if sentence[idx] in names_vocab:\n",
    "        word_features['in_names'] = 1\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        word_features['in_names'] = 0\n",
    "    \n",
    "    if sentence[idx] in gaz_vocab:\n",
    "        word_features['in_gaz_vocab'] = 1\n",
    "\n",
    "    else:\n",
    "        word_features['in_gaz_vocab'] = 0\n",
    "        \n",
    "    if left_word in gaz_vocab:\n",
    "        word_features['left_in_gaz_vocab'] = 1\n",
    "        \n",
    "    else:\n",
    "        word_features['left_in_gaz_vocab'] = 0\n",
    "        \n",
    "#     if left_word2 in gaz_vocab:\n",
    "#         word_features['left2_in_gaz_vocab'] = 1\n",
    "        \n",
    "#     else:\n",
    "#         word_features['left2_in_gaz_vocab'] = 0\n",
    "\n",
    "        \n",
    "    if right_word in gaz_vocab:\n",
    "        word_features['right_in_gaz_vocab'] = 1\n",
    "        \n",
    "    else:\n",
    "        word_features['right_in_gaz_vocab'] = 0\n",
    "        \n",
    "    # More features\n",
    "    \n",
    "#     if postags[idx][1] == 'NNP':\n",
    "#         word_features['pos_nnp'] = 1\n",
    "#     else:\n",
    "#         word_features['pos_nnp'] = 0\n",
    "    \n",
    "    word_features['pos_tag'] = postags[idx][1]\n",
    "    \n",
    "    if left_word != pad_token:\n",
    "        word_features['left_pos_tag'] = postags[idx - 1][1]\n",
    "        \n",
    "    if left_word2 != pad_token:\n",
    "        word_features['left2_pos_tag'] = postags[idx - 2][1]\n",
    "        \n",
    "    if right_word != pad_token:\n",
    "        word_features['right_pos_tag'] = postags[idx + 1][1]\n",
    "        \n",
    "    if right_word2 != pad_token:\n",
    "        word_features['right2_pos_tag'] = postags[idx + 2][1]\n",
    "        \n",
    "    \n",
    "    return word_features\n",
    "    \n",
    "def sentence2features(sentence, postags):\n",
    "    return [word2features(sentence, idx, postags) for idx in range(len(sentence))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_feature_dicts(ner_files, sent_level = False):\n",
    "    '''ner_files is a list of Ontonotes files with NER annotations. Returns feature dictionaries and \n",
    "    IOB tags for each token in the entire dataset'''\n",
    "    \n",
    "    train_dicts = []\n",
    "    train_tags = []\n",
    "\n",
    "    for filename in ner_files:\n",
    "        with open(ontonotes_path + filename, encoding=\"utf-8\") as f:\n",
    "            f.readline()\n",
    "            for sentence in f:\n",
    "                curr_tokens, curr_tags = sentence2iob(sentence)\n",
    "                \n",
    "                postags = nltk.pos_tag(curr_tokens)\n",
    "                \n",
    "                sentence_features = sentence2features(curr_tokens, postags)\n",
    "                if sent_level:\n",
    "                    train_dicts.append(sentence_features)\n",
    "                    train_tags.append(curr_tags)\n",
    "                else:\n",
    "                    train_dicts.extend(sentence_features)\n",
    "                    train_tags.extend(curr_tags)\n",
    "                \n",
    "                \n",
    "\n",
    "    return train_dicts, train_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use features to train a Multinomial Naive Bayes classifer, with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dicts, train_tags = prepare_ner_feature_dicts(train_data)\n",
    "dev_dicts, dev_tags = prepare_ner_feature_dicts(dev_data)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "train_features = vectorizer.fit_transform(train_dicts)\n",
    "dev_features = vectorizer.transform(dev_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(train_features, train_tags)\n",
    "dev_pred = classifier.predict(dev_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3368414413105045"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(dev_tags,dev_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92594507573532"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(dev_tags,dev_pred,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/terence/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   B-CARDINAL       0.59      0.56      0.57      1216\n",
      "       B-DATE       0.74      0.59      0.65      2230\n",
      "      B-EVENT       1.00      0.02      0.03       130\n",
      "        B-FAC       0.00      0.00      0.00       149\n",
      "        B-GPE       0.79      0.86      0.82      2738\n",
      "   B-LANGUAGE       0.00      0.00      0.00       114\n",
      "        B-LAW       0.00      0.00      0.00        47\n",
      "        B-LOC       0.77      0.07      0.13       231\n",
      "      B-MONEY       0.73      0.64      0.68       712\n",
      "       B-NORP       0.85      0.72      0.78       928\n",
      "    B-ORDINAL       0.74      0.25      0.37       222\n",
      "        B-ORG       0.87      0.36      0.51      3024\n",
      "    B-PERCENT       0.71      0.69      0.70       574\n",
      "     B-PERSON       0.65      0.79      0.71      2082\n",
      "    B-PRODUCT       1.00      0.04      0.08       101\n",
      "   B-QUANTITY       1.00      0.01      0.02       125\n",
      "       B-TIME       0.50      0.01      0.02       203\n",
      "B-WORK_OF_ART       0.00      0.00      0.00        67\n",
      "   I-CARDINAL       0.85      0.04      0.08       428\n",
      "       I-DATE       0.82      0.67      0.74      2615\n",
      "      I-EVENT       1.00      0.03      0.05       307\n",
      "        I-FAC       1.00      0.02      0.04       254\n",
      "        I-GPE       0.87      0.50      0.64       689\n",
      "   I-LANGUAGE       0.00      0.00      0.00         9\n",
      "        I-LAW       1.00      0.03      0.06       157\n",
      "        I-LOC       0.72      0.13      0.21       222\n",
      "      I-MONEY       0.88      0.84      0.86      1464\n",
      "       I-NORP       0.00      0.00      0.00        53\n",
      "        I-ORG       0.82      0.58      0.68      4146\n",
      "    I-PERCENT       0.97      0.77      0.86       801\n",
      "     I-PERSON       0.77      0.33      0.46      1429\n",
      "    I-PRODUCT       1.00      0.04      0.08        91\n",
      "   I-QUANTITY       1.00      0.10      0.18       216\n",
      "       I-TIME       0.88      0.14      0.25       263\n",
      "I-WORK_OF_ART       0.00      0.00      0.00       165\n",
      "            O       0.94      1.00      0.97    141996\n",
      "\n",
      "     accuracy                           0.92    170198\n",
      "    macro avg       0.68      0.30      0.34    170198\n",
      " weighted avg       0.92      0.92      0.91    170198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_tags, dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "In macroaveraged f-score, class balance is not considered. As shown in the reports, the major class is \"O\", which should give more weights during averaging. The microaveraged f-score considers the class balance in the dataset,\n",
    "which gives more weights to larger classes like \"O\", and the returned f-score is much higher. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with using a regular (non-sequential) classifier for `IOB-based NER` is that it may create ill-formed named entities, i.e. `I-` tags with no corresponding `B-` or `I-` tags before it. Check how often this is happening in the dev set with your classifier (the answer is \"a lot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053749162739867684\n"
     ]
    }
   ],
   "source": [
    "ill_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for i, pred in enumerate(dev_pred):\n",
    "    total_count += 1\n",
    "    if pred.startswith(\"B-\"):\n",
    "        curr_type = pred\n",
    "        \n",
    "    if pred.startswith(\"I-\"):\n",
    "        if dev_pred[i-1].startswith(\"B-\") or dev_pred[i-1].startswith(\"I-\"):\n",
    "            if pred != curr_type:\n",
    "                ill_count += 1\n",
    "                \n",
    "        else:\n",
    "            ill_count += 1\n",
    "            \n",
    "Percentage = ill_count/total_count\n",
    "print(Percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9148"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ill_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a CRF\n",
    "\n",
    "train a CRF model using the `sklearn_crfsuite` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dicts, train_tags = prepare_ner_feature_dicts(train_data, sent_level = True)\n",
    "dev_dicts, dev_tags = prepare_ner_feature_dicts(dev_data, sent_level = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7087665903819654\n",
      "0.9690125618397396\n"
     ]
    }
   ],
   "source": [
    "crf = CRF(algorithm = 'l2sgd', max_iterations = 15, c2 = 1e-5)\n",
    "crf.fit(train_dicts, train_tags)\n",
    "dev_pred = crf.predict(dev_dicts)\n",
    "print(flat_f1_score(dev_tags,dev_pred,average='macro'))\n",
    "print(flat_f1_score(dev_tags,dev_pred,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the top and bottom 10 transitions in terms of weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 10: [('B-PERCENT', 'O'), ('B-LANGUAGE', 'O'), ('B-PERSON', 'B-PERSON'), ('B-PERSON', 'B-ORG'), ('B-PERSON', 'B-GPE'), ('B-GPE', 'B-PERSON'), ('B-PERSON', 'B-NORP'), ('B-LAW', 'O'), ('I-ORDINAL', 'O'), ('I-ORG', 'B-NORP')]\n",
      "top 10: [('B-ORG', 'I-ORG'), ('B-LAW', 'I-LAW'), ('I-QUANTITY', 'I-QUANTITY'), ('B-PRODUCT', 'I-PRODUCT'), ('B-CARDINAL', 'I-CARDINAL'), ('B-MONEY', 'I-MONEY'), ('B-DATE', 'I-DATE'), ('B-PERCENT', 'I-PERCENT'), ('B-QUANTITY', 'I-QUANTITY'), ('B-TIME', 'I-TIME')]\n"
     ]
    }
   ],
   "source": [
    "transition_weights = crf.transition_features_\n",
    "\n",
    "sorted_weights = sorted(transition_weights, key = transition_weights.get)\n",
    "print(\"bottom 10:\",sorted_weights[:10])\n",
    "print(\"top 10:\",sorted_weights[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transition weights make sense.\n",
    "\n",
    "For the bottom 10s:\n",
    "\n",
    "- It is likely that `percentages` and `language` would contain more than 1 token, like `50 %` contains 2 tokens, so the transition from the `B-` to `O` for these types are less likely to happen.\n",
    "- Also, it is less likely that two differnet `B-` tags are placed together because we are less likely to mention several names from different aspects simultaneously.\n",
    "\n",
    "For the top 10s:\n",
    "\n",
    "- These are tags that are in the same type, so they are very likely to be in the sequential order like this, so high transition weights for them are reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Competition Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = [ontonotes_path + 'kaggle_untagged/' + file for file in os.listdir(ontonotes_path + 'kaggle_untagged/')]\n",
    "kaggle_data = sorted(kaggle_data) #ensures the files are in a standard order for consistency\n",
    "header = 'Id,Predicted\\n'\n",
    "\n",
    "kaggle_dicts = []\n",
    "for file in kaggle_data:\n",
    "    with open(file) as f:\n",
    "        for sentence in f:\n",
    "            curr_tokens = sentence.strip().split(\" \")\n",
    "\n",
    "            postags = nltk.pos_tag(curr_tokens)\n",
    "                \n",
    "            sentence_features = sentence2features(curr_tokens, postags)\n",
    "            kaggle_dicts.append(sentence_features)\n",
    "    \n",
    "\n",
    "kaggle_pred = crf.predict(kaggle_dicts)\n",
    "\n",
    "curr_idx = 0\n",
    "with open('./kaggle_tags.csv', 'w') as f:\n",
    "    f.write(header)\n",
    "    for tags in kaggle_pred:\n",
    "        for c in tags:\n",
    "            f.write(str(curr_idx) + \",\" + c + \"\\n\")\n",
    "            curr_idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "with open('kaggle_tags.csv') as f:\n",
    "    lines = f.readlines()\n",
    "    assert len(lines) == 306118\n",
    "    assert lines[0] == header\n",
    "    assert lines[1].startswith(\"0,\")\n",
    "    assert lines[-1].startswith(\"306116,\")    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle competition [here](https://www.kaggle.com/c/ubc-mdscl-colx563-ner/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
